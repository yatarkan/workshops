{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<center> <h1>OpenVINO™ Deep Learning Workbench: NLP TITLE</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Introduction](#intro)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#OV-overview)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#DL-WB-overview)\n",
    "\n",
    "## 4. [OpenVINO™ API](#OV-API)\n",
    "\n",
    "## 5. [Practice](#tasks)\n",
    "\n",
    "## 6. [Bonus: Deploy your first OpenVINO application as a telegram bot](#bot)\n",
    "\n",
    "## 7. [Bonus: Next steps with OpenVINO](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction  <a name=\"intro\"></a>\n",
    "\n",
    "### Workshop Contributors\n",
    "\n",
    "<div style=\"display: block; text-align: center;\">\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Demidovskij.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Alexander Demidovskij</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/demid5111\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@demid5111</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Artur Paniukov</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/apaniukov\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@apaniukov</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "    <figure style=\"display: inline-block;\">\n",
    "      <img style=\"border-radius: 50%;\" src=\"pictures/Savina.jpg\" width=\"150\" height=\"150\"/>\n",
    "      <figcaption style=\"text-align: center; font-weight: bold;\">Tatiana Savina</figcaption>\n",
    "      <figcaption style=\"text-align: center\">\n",
    "        <a href=\"https://github.com/tsavina\" style=\"display: inline-flex;\"><img src=\"./pictures/github.svg\" width=\"22px\" style=\"margin-right: 5px\">@tsavina</a>\n",
    "      </figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In This Workshop\n",
    "\n",
    "Welcome to the Deep Learning workshop, where you will find out how to start working with pre-trained neural networks and create a transformer-based text classifier applying model optimization techniques. For that, we will be using OpenVINO™ framework and its graphical interface Deep Learning Workbench. \n",
    "\n",
    "During this workshop, you will:\n",
    "\n",
    "1. Learn the basics of neural model analysis and optimization:\n",
    "    - what a model is and how it works\n",
    "    - how model inference works\n",
    "    - how to measure model performance \n",
    "    - how to tune the model for enhanced performance\n",
    "2. Write your own AI application that detects toxic messages.\n",
    "\n",
    "###  NLP and Deep Learning\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of technology that helps programs understand and interpret human language. NLP fills the gap between human communication and machine comprehension. With the rapid development of the NLP field, deep learning models achieve state-of-the-art performance in various business problems such as question answering, sentence comparison, or text summarization. According to IBM's [Global AI Adoption Index 2021](https://newsroom.ibm.com/IBMs-Global-AI-Adoption-Index-2021?lnk=ushpv18ai3), about half of companies are already employing NLP-enabled applications, and more than a quarter plan to do so within the following year. \n",
    "\n",
    "![](nlp_img/graph1.png)\n",
    "![](nlp_img/graph2.png)\n",
    "\n",
    "Typically, NLP algorithms evaluate vast amounts of unstructured text data, such as documents, log files, transcripts, etc. The output of an NLP model might vary based on its objective. Digital Assistants like Alexa (Amazon), Siri (Apple), Cortana (Microsoft), and Google Assistant use NLP to identify speech patterns and infer meaning or complete a task to assist the user. Recent advances have enabled the NLP field to evolve from tools like spell check and spam filter to more sophisticated apps such as NLP-powered chatbots, real-time voice-to-text translators, and other services that can conduct phone calls, track conversations and collect important data from chats and emails, and many more.\n",
    "\n",
    "Along with the achieved groundbreaking results, the size of these models has skyrocketed, reaching millions (or even billions) of parameters, combined with increasing growth in complexity. Therefore, there is a strong need to accelerate neural models inference so that they can meet demanding performance and accuracy requirements in production. Before we try one of the acceleration techniques, let's define the model inference and find out how it differs from the model training. \n",
    "\n",
    "###  Text Model Inference\n",
    "\n",
    "To perform a specific AI task, the model is trained on the known data. During training, the model makes predictions about what the data represents. Any error in the prediction is used to strengthen the artificial neuron connections until the acceptable accuracy level is achieved. The process of training usually done just once and requires massive amount of data and powerful computing systems.\n",
    "\n",
    "The process of using a pre-trained model to make predictions against previously unseen data is called inference. Inference is performed on different devices, happens several times, and consists in feeding the data to the model. In other words, with NLP technologies inferences happen almost all the time, and model output is used by companies and apps for speech recognition, search autocomplete, spam filter, recommendation systems, etc. \n",
    "\n",
    "![](nlp_img/nlp-deep-learning.png)\n",
    "\n",
    "In this workshop we will work with model inference; in other words, we will execute the model already pre-trained to perform useful work. Let's take a look at the inference on the real-life example of the toxic message detection model.\n",
    "\n",
    "Web services and apps that handle large numbers of user messages might profit from automated moderation systems. Potentially, such systems could reduce the cost and time required for manual moderation while increasing the process efficiency. Suppose we have online chats where messages are created by hundreds of users every minute. The messages are processed on the server, and the task of our NLP model is to filter out potentially toxic communications before they reach the chat interface. The model must respond quickly enough so that the structure of the users' conversation is not disrupted.\n",
    "\n",
    "In this case, inference consists in: \n",
    "\n",
    "-\ttaking a text message as input\n",
    "-\ttokenizing it into numerical data\n",
    "-\tfeeding that data to our model\n",
    "-\treturning a classification label (toxic/non-toxic) produced by the model\n",
    "\n",
    "![](nlp_img/inference_example.png)\n",
    "\n",
    "Besides making NLP models fast enough for users, we need to accelerate them and make economical enough to run in production at a manageable cost. As a result, there is a challenge of accelerating inference of usually complex and large NLP models in the absence of any expensive specialized hardware.\n",
    "\n",
    "So let's find out how the OpenVINO™ toolkit can help in achieving better performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenVINO™ Toolkit <a name=\"OV-overview\"></a>\n",
    "\n",
    "1. [How OpenVINO advances AI technologies](#customers) \n",
    "2. [Introduction to the Openvino toolkit](#OV-intro) \n",
    "3. [OpenVINO capabilities](#OV-capabilities)\n",
    "4. [OpenVINO components](#OV-components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 How OpenVINO advances AI technologies <a name=\"customers\"></a>\n",
    "\n",
    "The Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit is a comprehensive toolkit for optimizing pre-trained Deep Learning models of various use cases to achieve high performance and prepare them for deployment on Intel® platforms. Based on latest generations of artificial neural networks, including convolutional neural networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud.\n",
    "\n",
    "![](nlp_img/case_studies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to the OpenVINO™ toolkit <a name=\"OV-intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_motivation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 OpenVINO™ Capabilities <a name=\"OV-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/ov_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 OpenVINO™ Toolkit Components <a name=\"OV-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenVINO™ Open Model Zoo <a name=\"OMZ\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nlp_img/omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Workbench: OpenVINO™ Quickstart <a name=\"DL-WB-overview\"></a>\n",
    "\n",
    "1. [DL Workbench Capabilities](#DL-WB-capabilities)\n",
    "2. [DL Workbench Workflow](#DL-WB-workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is the official OpenVINO™ graphical interface designed to make the production of pre-trained deep learning models significantly easier.\n",
    "With DL Workbench you can start working with your deep learning model right from your browser: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DL Workbench Capabilities <a name=\"DL-WB-capabilities\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nlp_img//DL-WB-flow.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "\n",
    "model_checkpoint = \"SkolkovoInstitute/russian_toxicity_classifier\"\n",
    "model_name = model_checkpoint.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\",\n",
    "    model=model_checkpoint,\n",
    "    output=onnx_model_path,\n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",  # default pipeline name for text classification and textual entaimnet models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DL Workbench Workflow <a name=\"DL-WB-workflow\"></a>\n",
    "\n",
    "\n",
    "1. [Open DL Workbench](#open-wb)\n",
    "2. [Import the Model](#import-model)\n",
    "3. [Import the Dataset](#import-dataset) \n",
    "4. [Benchmark the Model](#model-inference)\n",
    "5. [Analyze the Model](#analyze-model) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Download [russian_toxicity_classifier](https://huggingface.co/SkolkovoInstitute/russian_toxicity_classifier) model in ONNX format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "\n",
    "model_checkpoint = \"SkolkovoInstitute/russian_toxicity_classifier\"\n",
    "model_name = model_checkpoint.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx_model/{model_name}.onnx\")\n",
    "if onnx_model_path.parent.exists():\n",
    "    rmtree(onnx_model_path.parent)\n",
    "\n",
    "convert(\n",
    "    framework=\"pt\",\n",
    "    model=model_checkpoint,\n",
    "    output=onnx_model_path,\n",
    "    opset=12,  # check other opsets or try one more time if conversion fails\n",
    "    pipeline_name=\"sentiment-analysis\",  # default pipeline name for text classification and textual entaimnet models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Download dataset. \n",
    "\n",
    "You will need to obtain the data to work with the model. The data depends on the task for which the model has been trained. Learn more in the [documentation](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Dataset_Types.html). \n",
    "\n",
    "In our case, we take a set of toxic and non-toxic messages and use them to create our validation dataset.\n",
    "\n",
    "Use the following **link to download the dataset**: [Download Dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display button for opening DL Workbench\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"<button class=\"wb-button\" onclick=\"window.open(location.origin, '_blank');\">Open DL Workbench</button><style>.wb-button { display: flex; width: fit-content; margin: 20px auto; align-items: center; height: 50px; font-size: 18px; font-weight: 400; font-family: inherit; line-height: 20px; background-color: #003C71; border: 1px solid #003C71; color: #ffffff; cursor: pointer; border-radius: 4px; padding: 0 30px; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.15);}</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open DL Workbench <a name=\"open-wb\"></a>\n",
    "\n",
    "To start working with DL Workbench, click **Create Project** button to open the Create Project page.\n",
    "\n",
    "![](nlp_img/start_page_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Model <a name=\"import-model\"></a>\n",
    "\n",
    "Our first step is to **Import the model**. \n",
    "\n",
    "![](nlp_img/create_project_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **Original Model** tab, specify **NLP** domain and **ONNX** framework. Select and upload `.onnx` model file and click **Import**: \n",
    "\n",
    "![](nlp_img/d_import_wizard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with OpenVINO tools, you need to obtain a model in Intermediate Representation (IR) format. \n",
    "IR is the OpenVINO format of pre-trained model representation with two files:\n",
    "\n",
    "- XML file describing the network topology\n",
    "- BIN file containing weights and biases\n",
    "\n",
    "On the third stage **Convert Model to IR - General Parameters**, click **Convert** to proceed:\n",
    "\n",
    "![](nlp_img/d_convert_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure model layout, set **NC layout**, **1 batch** and **128 channels** for each input. Click **Validate and Import**:\n",
    "\n",
    "![](nlp_img/d_layput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is successfully imported, you will see it on the Create Project page. Click on the model to select it:\n",
    "\n",
    "![](nlp_img/d_model_ready.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed and click **Next Step**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Environment <a name=\"import-dataset\"></a>\n",
    "\n",
    "On the **Select Environment** page you can choose a hardware accelerator on which the model will be executed. We will analyze how our model works on CPU since we have this device available. Go to the **Next Step**.\n",
    "\n",
    "![](nlp_img/d_select_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset <a name=\"import-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " To **Select Validation Dataset**, click **Import Text Dataset** button.\n",
    "\n",
    "![](nlp_img/d_text_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset file, make sure it is displayed correctly and click **Import**.\n",
    "\n",
    "![](nlp_img/d_text_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Model <a name=\"model-inference\"></a>\n",
    "\n",
    "Now that we have imported our model, we want to check how fast it works. For that, let's create our first project. Select the model and the dataset by clicking on them, and click **Create Project**.\n",
    "\n",
    "![](nlp_img/create_project_completed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model <a name=\"analyze-model\"></a>\n",
    "\n",
    "When the inference stage is finished, we can see the result of running our model on the CPU. \n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)\n",
    "\n",
    "Latency is the time required to process the data. The lower the value, the better. Throughput shows how much data the model processes per second. Higher throughput value means better performance. \n",
    "\n",
    "Under the table with results you see a hint saying the model was inferred on the autogenerated data. To infer the model on the text dataset, you need to use a tokenizer. Click **Import Tokenizer** link in the hint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Select Tokenizer <a name=\"import-dataset\"></a>\n",
    "\n",
    "Tokenizers are used to convert text to numerical data because the model cannot work with the text directly.To work with the model using text from the imported dataset, you need to:\n",
    "\n",
    "- Import tokenizer:\n",
    "\n",
    "Select **WordPiece** as tokenizer type, upload `vocab.txt` file, and click **Import**.\n",
    "\n",
    "![](nlp_img/d_import_tokenizer.png)\n",
    "\n",
    "- Select tokenizer:\n",
    "\n",
    "Click on the tokenizer to select it.\n",
    "\n",
    "![](nlp_img/d_select_tokenizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile the Model <a name=\"profile-model\"></a>\n",
    "\n",
    "\n",
    "Open the **Project** tab and go back to the model project:\n",
    "\n",
    "![](nlp_img/d_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Select **Perform** and open **Explore Inference Configurations** tab. \n",
    "\n",
    "You can accelerate your model by configuring the optimal inference parameters specific to each accelerator: streams and batches.\n",
    "Streams are the number of instances of your model running simultaneously, and batches are the number of input data instances fed to the model. Let's first infer our model with default **1 Stream** and **1 Batch** parameters.\n",
    "\n",
    "![](nlp_img/d_perform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the model inference obtained on the messages from our dataset.\n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run a range of inference streams to find the optimal configurations and accelerate your model. Go to the **Perform** tab, open **Explore Inference Configurations** subtab, select Group Inference and click **Configure Group Inference**. On the Configure Group Inference page, select combinations of stream and batch parameters by clicking corresponding cells in the table and click **Execute**.\n",
    "\n",
    "![](nlp_img/d_execute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results. Remember that higher throughput value means better performance. \n",
    "\n",
    "![](nlp_img/d_benchmark_results.png)\n",
    "\n",
    "Our model became Xtime faster with X batch X stream configurations. Furhter, we will use this information to create our working application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s briefly recap what you have learned at this stage:\n",
    "\n",
    "1. What a model is and how it works\n",
    "2. What model inference is and why it is important to accelerate its speed\n",
    "3. How to measure and improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learn OpenVINO™ API <a name=\"OV-API\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's learn how to infer a model of text classification use case with OpenVINO™ Python interface and build our application. Text classification is a task of predicting the class of the given text..\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "1. [Obtain Required Modules](#1.-Obtain-Required-Modules) \n",
    "2. [_Optional_. Download and convert a pretrained model from the Open Model Zoo](#2.-Optional.-Download-and-Convert-a-Pretrained-Model-from-the-Open-Model-Zoo)\n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Display results](#9.-Display-Results)\n",
    "10. [_Optional_. Compare OpenVINO and PyTorch model](#10.-Optional.-Compare-OpenVINO-and-PyTorch-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 128\n",
    "\n",
    "assert sequence_length <= tokenizer.model_max_length\n",
    "\n",
    "inputs = \",\".join(tokenizer.model_input_names)\n",
    "input_shapes = \",\".join(f\"[{batch_size},{sequence_length}]\" for _ in range(len(tokenizer.model_input_names)))\n",
    "\n",
    "print(\n",
    "    f\"Model inputs: {inputs}\\n\"\n",
    "    f\"Input shapes {input_shapes}\"\n",
    ")\n",
    "\n",
    "openvino_model_dir = Path(\"ir_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s \"$onnx_model_path\" \"$inputs\" \"$input_shapes\" \"$openvino_model_dir\"\n",
    "# source /tmp/virtualenvs/onnx/bin/activate\n",
    "\n",
    "# export PYTHONPATH=/home/artur_paniukov/python/fork/workbench/.unified_venv/lib/python3.8/site-packages:$PYTHONPATH\n",
    "\n",
    "# mo --input_model \"$1\" --input \"$2\" --input_shape \"$3\" --output_dir \"$4\" --data_type \"FP16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: If you have the model imported in DL Workbench, copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "model_xml = openvino_model_dir / f\"{model_name}.xml\"\n",
    "model_bin = openvino_model_dir / f\"{model_name}.bin\"\n",
    "\n",
    "dataset_path = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "print(\n",
    "    \"Configuration parameters settings:\"\n",
    "    f\"\\n\\tmodel_xml={model_xml}\",\n",
    "    f\"\\n\\tmodel_bin={model_bin}\",\n",
    "    f\"\\n\\tdevice={device}\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `Core` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "model = core.read_model(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Set input tensor names\n",
    "for inp, name in zip(model.inputs, tokenizer.model_input_names):\n",
    "    inp.get_tensor().set_names({name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps.\n",
    "\n",
    "After loading, we keep necessary model information - `output_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=device)\n",
    "\n",
    "output_name = model.output().any_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Text for Inference\n",
    "\n",
    "The model cannot work with the text directly. Instead, the text is first split into tokens and then replaces each token with the corresponding index. A token can be a word, part of a word, a symbol, or a couple of symbols. The map between tokens and indices is stored by the tokenizer.\n",
    "\n",
    "The model input that takes token indices is usually called `input_ids`. There are also might be other inputs. If the model input is static, meaning that it can take only fixed-size inputs, one could have to make an input text longer. For that, there is a special token, called _padding_, that can be added to the beginning or to the end of the sequence. To ignore these tokens during inference there is an `attention_mask` model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Мне фильм понравился\"]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    input_text,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=sequence_length,\n",
    "    return_tensors=\"np\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cpu_config = {'PERF_COUNT': 'NO', 'PERFORMANCE_HINT': 'LATENCY', 'CPU_THROUGHPUT_STREAMS': '1'}\n",
    "core.set_config(cpu_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the starting time\n",
    "inf_start = time.time()\n",
    "\n",
    "# Run the inference\n",
    "res = compiled_model.infer_new_request(dict(**tokenized_text))[0]\n",
    "\n",
    "# Calculate the time from the start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(f\"Inference is complete. Run time: {inf_time * 1000:.3f} ms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res, res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "Now `res` contains the result of the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try to load index to label map from model config\n",
    "model_config = PretrainedConfig.from_pretrained(model_checkpoint)\n",
    "idx_to_label_dict = getattr(model_config, \"id2label\", {})\n",
    "\n",
    "idx_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that can be applied to vectors\n",
    "# if there is no `idx_to_label` map, the function returns class index\n",
    "idx_to_label = np.vectorize(lambda x: idx_to_label_dict.get(x, x))\n",
    "\n",
    "\n",
    "def process_result(res):\n",
    "    # take the position of the maximum element in the vector to get the predicted class index\n",
    "    predicted_class_idx = ...\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    \n",
    "    # transform class index to class label\n",
    "    return idx_to_label(predicted_class_idx)\n",
    "\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stream/batch inference\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine inference and result processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        pad_to_multiple_of=sequence_length,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    res = compiled_model.infer_new_request(dict(**tokenized_text))[0]\n",
    "    predicted_class_idx = np.argmax(res, axis=1)\n",
    "    return idx_to_label(predicted_class_idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict(\"Шел бы ты отсюда...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Predict labels for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"predicted\"] = dataset[\"text\"].apply(predict)\n",
    "dataset.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you can proceed to importing the model into the DL Workbench or if you have already done that, start exploring numerous features such as:\n",
    "\n",
    "* [Analyse how the model works and its quality](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Visualize_Accuracy.html)\n",
    "* [Perform a baseline inference and analyze model performance](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Single_Inference.html)\n",
    "* [Boost the model by calibrating it to the INT8 precision](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Int_8_Quantization.html)\n",
    "* [Tune the performance of the model by selecting optimal inference parameters](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Run_Range_of_Inferences.html)\n",
    "* [Preparing the model for deployment](https://docs.openvino.ai/latest/workbench_docs_Workbench_DG_Deploy_and_Integrate_Performance_Criteria_into_Application.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice <a name=\"tasks\"></a>\n",
    "\n",
    "1. [Task 1: ](#task1) \n",
    "2. [Task 2: ](#task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:  <a name=\"task1\"></a>\n",
    "\n",
    "The goal of this task is to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = twint.Config()\n",
    "config.Limit = 1\n",
    "config.Username = \"vas3k\"\n",
    "\n",
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  <a name=\"task2\"></a>\n",
    "\n",
    "Now that we have learned how to ...., let's try to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploy your first OpenVINO application as a telegram bot <a name=\"bot\"></a>\n",
    "\n",
    "\n",
    "When you find an optimal configuration for your model, the next step is to use this model with optimal parameters in your own application on a target device. OpenVINO™ toolkit includes all you need to run the application on the target. However, the target might have a limited drive space to store all OpenVINO™ components. OpenVINO™ Deployment Manager available inside the DL Workbench extracts the minimum set of libraries required for a target device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Next steps with OpenVINO <a name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try Workbench right now: locally or in DevCloud\n",
    "\n",
    "![](workshops/cv/pictures/call-to-action.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Intel® Edge AI Certification](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/learn/certification.html)\n",
    "\n",
    "![](pictures/edge-ai-certification.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_notebooks",
   "language": "python",
   "name": "openvino_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}